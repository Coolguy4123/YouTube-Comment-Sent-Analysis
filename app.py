import streamlit as st
from googleapiclient.discovery import build
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
import tensorflow as tf
import numpy as np
import re
from dotenv import load_dotenv
import os

# === Your YouTube Data API key ===
load_dotenv()
YOUTUBE_API_KEY = os.getenv("API_KEY")  # Or replace with string for testing

# === Load your fine-tuned model ===
@st.cache_resource
def load_model_and_tokenizer():
    model = TFAutoModelForSequenceClassification.from_pretrained("roberta_web_model")
    tokenizer = AutoTokenizer.from_pretrained("roberta_web_model")
    return model, tokenizer

model, tokenizer = load_model_and_tokenizer()

# === Utility: Extract video ID from URL ===
def extract_video_id(url):
    match = re.search(r"(?:v=|\/)([0-9A-Za-z_-]{11})", url)
    return match.group(1) if match else None

# === Fetch comments from YouTube API ===
def fetch_comments(video_id, max_comments=1000):
    youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)
    comments = []
    request = youtube.commentThreads().list(
        part="snippet",
        videoId=video_id,
        maxResults=100,
        textFormat="plainText"
    )
    response = request.execute()
    while response and len(comments) < max_comments:
        for item in response.get("items", []):
            comment = item["snippet"]["topLevelComment"]["snippet"]["textDisplay"]
            comments.append(comment)
            if len(comments) >= max_comments:
                break
        request = youtube.commentThreads().list_next(request, response)
        response = request.execute() if request else None
    return comments

# === Run inference ===
def predict_sentiment(comments):
    inputs = tokenizer(comments, return_tensors="tf", padding=True, truncation=True, max_length=128)
    dataset = tf.data.Dataset.from_tensor_slices(inputs).batch(32)

    predictions = []
    for batch in dataset:
        outputs = model(batch)
        probs = tf.nn.softmax(outputs.logits, axis=1).numpy()
        predictions.append(probs)

    probs = np.concatenate(predictions, axis=0)
    preds = np.argmax(probs, axis=1)
    labels = ['negative', 'neutral', 'positive']
    results = [
        {"comment": comment, "sentiment": labels[preds[i]], "confidence": float(probs[i][preds[i]])}
        for i, comment in enumerate(comments)
    ]
    return results

# === Streamlit UI ===
st.title("üé• YouTube Comment Sentiment Analyzer")

url = st.text_input("Enter a YouTube video URL:")

if st.button("Fetch and Analyze Comments"):
    video_id = extract_video_id(url)
    if not video_id:
        st.error("‚ùå Invalid YouTube URL")
    else:
        with st.spinner("Fetching comments..."):
            comments = fetch_comments(video_id)
        if not comments:
            st.warning("No comments found.")
        else:
            st.success(f"Fetched {len(comments)} comments.")
            with st.spinner("Analyzing sentiment..."):
                results = predict_sentiment(comments)

            st.write("### üí° Results:")
            for r in results:
                st.markdown(f"üí¨ **{r['comment']}**")
                st.write(f"‚Üí Sentiment: **{r['sentiment']}** (Confidence: {r['confidence']:.2f})")
                st.markdown("---")